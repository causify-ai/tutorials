{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Tutorial For Langchain\n",
    "\n",
    "The notebook shows how to use langchain API with an example. In this example we will be building chatbot for the internal documentation using lancgchain.\n",
    "\n",
    "Refernces: \n",
    " - Official docs : https://python.langchain.com/docs/introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4159763-8d4e-48ef-a8cb-8c22ec36da0d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532dfea4-93b4-41e2-b6c1-5d7f57a4140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "import langchain_openai as langOpenAI\n",
    "import langchain.document_loaders as docloader\n",
    "import langchain.docstore.document as docstore\n",
    "import langchain.text_splitter as txtsplitter\n",
    "import langchain.embeddings as lang_embeddings\n",
    "import langchain.vectorstores as vectorstores\n",
    "import langchain.chains as chains\n",
    "import langchain.chat_models as chatmodels\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import helpers.hsystem as hsystem\n",
    "import helpers.hprint as hprint\n",
    "import helpers.hdbg as hdbg\n",
    "import helpers.hpandas as hpanda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63a3a326-140a-4543-8f01-155e0fa36192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING\u001b[0m: Logger already initialized: skipping\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "### Define the GPT Model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "chat_model = langOpenAI.ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_markdown_files(file_paths) -> List[docstore.Document]:\n",
    "    \"\"\"\n",
    "    Parse all the markdown files into Documents.\n",
    "\n",
    "    :param file_paths: list of md file_paths\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        # Create a Document object for each file\n",
    "        documents.append(docstore.Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2c959f-0def-4a38-b329-34bf7f3206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_markdown_files(directory:str) -> List[str]:\n",
    "    return list(glob.glob(f\"{directory}/*.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter \n",
    "Utility function in LangChain  for splitting large chunks of text into smaller more manageable pieces while ensuring minimal overlap or fragmentation of meaningful content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'<!-- toc -->\\n\\n- [Tutorials \"Learn X in 60 minutes\"](#tutorials-learn-x-in-60-minutes)\\n  * [What are the goals for each tutorial](#what-are-the-goals-for-each-tutorial)\\n\\n<!-- tocstop -->\\n\\n# Tutorials \"Learn X in 60 minutes\"\\n\\nThe goal is to give everything needed for one person to become familiar with a\\nBig data / AI / LLM / data science technology in 60 minutes.\\n\\n- Each tutorial conceptually corresponds to a blog entry.'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'Each tutorial corresponds to a directory in the `//tutorials` repo\\n[https://github.com/causify-ai/tutorials](https://github.com/causify-ai/tutorials)\\nwith'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'- A markdown \\\\`XYZ.API.md\\\\` about the API and the software layer written by us\\n  on top of the native API\\n- A markdown `XYZ.example.md` with a full example of an application using the\\n  API\\n- A Docker container with everything you need in our Causify dev-system format\\n- A Jupyter notebook with an example of APIs\\n- A Jupyter notebook with a full example\\n\\n## What are the goals for each tutorial\\n\\nDocker container'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'Docker container\\n\\n- Provides a Docker container with everything installed and ready to run\\n  tutorials and develop with that technology\\n  - Often installing the package and get it to work takes long to figure out\\n- All the code is on GitHub in a common format to all tutorials\\n\\nJupyter notebooks'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {\"- Each Jupyter notebook should\\n  - Be unit tested so that you are guaranteed that it works\\n    - It's super frustrating when a tutorial doesn't work because the version of\\n      the library is not compatible with the code anymore\\n  - Be self-contained and linear: each example is explained thoroughly without\\n    having to jump from tutorial to tutorial\\n    - Each cell and its output is commented and explained\\n  - Run end-to-end after a restart (we can add a unit test for it)\"}\n"
     ]
    }
   ],
   "source": [
    "# Directory containing Markdown files\n",
    "directory = \"../../docs\"\n",
    "\n",
    "# List Markdown files\n",
    "markdown_files = list_markdown_files(directory)\n",
    "\n",
    "# Parse Markdown files into LangChain documents\n",
    "documents = parse_markdown_files(markdown_files)\n",
    "\n",
    "# Split long documents into smaller chunks\n",
    "text_splitter = txtsplitter.RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print sample chunked documents\n",
    "for doc in split_documents[:5]:\n",
    "    _LOG.info(\"Source: %s\", {doc.metadata['source']})\n",
    "    _LOG.info(\"Content: %s\", {doc.page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "### VECTOR STORES\n",
    "\n",
    "#### FAISS (Facebook AI Similarity Search) \n",
    "It is a library designed for efficient similarity search and clustering of dense vectors. In LangChain, FAISS is commonly used as a vector store to store and retrieve embeddings, which are vector representations of text or other data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0be86c16-f178-42cf-b0c0-911c84c0d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = lang_embeddings.OpenAIEmbeddings()\n",
    "\n",
    "# Embed and store split_documents\n",
    "vector_store = vectorstores.FAISS.from_documents(split_documents, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20a4a0dc-967f-4225-ba19-54f444498bac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the QA chain\n",
    "qa_chain = chains.RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8920b1c6-1f29-4092-a4ce-672fdc339033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_318/125407993.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO  Answer: The guidelines for creating a new project based on the Markdown documents should cover various aspects such as the package description, problem it solves, alternatives, native API description, Docker container details, visual aids using tools like mermaid, references to relevant resources, and ensuring that Jupyter notebooks are unit tested, self-contained, and run end-to-end after a restart. These guidelines aim to provide a comprehensive and well-documented approach to developing projects in areas like Git, Docker, databases, workflow management tools, and more.\n",
      "INFO  \n",
      "Source Documents:\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'Markdown documents should cover:'}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'This is the same approach we use in DATA605 tutorials\\nhttps://github.com/gpsaggese/umd\\\\_data605/tree/main/tutorials, e.g.,\\n\\n- Git\\n- Docker\\n- Docker compose\\n- Postgres\\n- MongoDB\\n- Airflow\\n- Dask\\n- GitH'}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'- What it is the package\\n- What problem it solves\\n- What are the alternatives, both open source and commercial with comments about\\n  advantages and disadvantages\\n- Describe the native API\\n- Descriptio'}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {\"- Each Jupyter notebook should\\n  - Be unit tested so that you are guaranteed that it works\\n    - It's super frustrating when a tutorial doesn't work because the version of\\n      the library is not com\"}\n"
     ]
    }
   ],
   "source": [
    "# User's question\n",
    "query = \"What are the guidelines on creating new project\"\n",
    "\n",
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "_LOG.info(\"Answer: %s\", result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "_LOG.info(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    _LOG.info(\"File: %s\", {doc.metadata['source']})\n",
    "    _LOG.info(\"Excerpt: %s\", {doc.page_content[:200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac00f5a8-4696-4604-96c7-f015fe482135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_by_document_name(vector_store: vectorstores.FAISS, document_name: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve vectors from a FAISS vector store based on the document name.\n",
    "\n",
    "    :param vector_store: FAISS vector store object that supports similarity search.\n",
    "    :param document_name:  name of the document used as a filter in the metadata.\n",
    "\n",
    "    :return: list of results from the FAISS vector store that match the given document name.\n",
    "    \"\"\"\n",
    "    # Query using the metadata field source\n",
    "    results = vector_store.similarity_search(\n",
    "        # Pass an empty query or a dummy vector if supported\n",
    "        query=\"\",\n",
    "        # Retrieve all matching documents\n",
    "        k=None,\n",
    "        # Filter by the document name\n",
    "        filter={\"source\": document_name} \n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a63c5db-d452-4392-bd4f-c68da5b3b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "document_name = \"all.how_write_tutorials.how_to_guide.md\"\n",
    "results = get_vectors_by_document_name(vector_store, document_name)\n",
    "\n",
    "# Print results\n",
    "for doc in results:\n",
    "    _LOG.info(\"File: %s\", {doc.metadata['source']})\n",
    "    _LOG.info(\"Content: %s\", {doc.page_content[:200]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68a944",
   "metadata": {},
   "source": [
    "### Demo to create a documentation QA bot but the docs can be updated or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2057dc61-2130-4395-ad63-9af8690698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some state.\n",
    "vector_store = None\n",
    "folder = \"../docs\"\n",
    "filename_to_md5sum = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8c7458e-e47e-4513-a5e0-caa3b5dae7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def parse_markdown_files(file_paths: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Parse and structure Markdown files into LangChain Document objects.\n",
    "\n",
    "    :param file_paths: list of file paths to the Markdown files\n",
    "\n",
    "    :return: list of Document objects, where each document contains the content\n",
    "                        of a Markdown file and metadata with the file's source path.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    filename_to_md5sum = {}\n",
    "    for file_path in file_paths:\n",
    "        # Read the content of the Markdown file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()       \n",
    "        # Compute the MD5 checksum of the file\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        filename_to_md5sum[file_path] = md5sum \n",
    "        # Create a Document object for each file\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85ae283e-6aff-49bc-ad21-fe8db669eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store_from_markdown_files(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(directory)\n",
    "    # Parse Markdown files into LangChain documents\n",
    "    documents = parse_markdown_files(markdown_files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = txtsplitter.RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    # Create embeddings for all documents.\n",
    "    vector_store = vectorstores.Chroma.from_documents(split_documents, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae0661b8-4e40-4e07-9cbd-b54cad041170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changes_in_documents_folder(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(folder)\n",
    "    changes = {}\n",
    "    changes[\"modified\"] = []\n",
    "    for file_path in markdown_files:\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        if file_path not in filename_to_md5sum or filename_to_md5sum[file_path] == md5sum:\n",
    "            print(f\"Found a new / modified file {file_path}\")\n",
    "            changes[\"modified\"].append(file_path)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5f35309-ca46-4a7e-9643-1943397dfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_in_vector_store(vector_store, files):\n",
    "    if len(files) == 0:\n",
    "        print(\"No new files found\")\n",
    "        return\n",
    "    ids_to_delete = []\n",
    "    for file in files:\n",
    "        for doc in vector_store:\n",
    "            if doc.metadata.get('source') == file:\n",
    "                ids_to_delete.append(doc.id)\n",
    "    vector_store.delete(ids_to_delete)\n",
    "    documents = parse_markdown_files(files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = txtsplitter.RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings_list = embeddings.embed_documents(texts)  # Compute embeddings for multiple documents\n",
    "    # Add documents to vector store with computed embeddings\n",
    "    vector_store.add_documents(\n",
    "        documents=split_documents,\n",
    "        embeddings=embeddings_list\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5be1a538-97ee-4bbf-b6ec-1ba96152fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the goals for tutorial project?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcb8ee5f-bb8f-4181-8925-63d1337bbe51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "if vector_store:\n",
    "    changes = get_changes_in_documents_folder(folder)\n",
    "    vector_store = update_files_in_vector_store(vector_store, changes[\"modified\"])\n",
    "else:\n",
    "    vector_store = create_vector_store_from_markdown_files(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10b93082-8410-4d6f-9d4c-21d115d21938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA chain\n",
    "qa_chain = chains.RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "675abf9f-b362-4d6a-8249-f8430971d2db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO  Answer: The goals for the tutorial project are to provide everything needed for one person to become familiar with a Big data / AI / LLM / data science technology in 60 minutes. This includes creating tutorials that are conceptually like blog entries, providing markdown files about the API and software layer, offering examples of applications using the API, supplying Docker containers in a specific format, and including Jupyter notebooks with examples of APIs and full examples. The tutorials should be unit tested, self-contained, linear, and run end-to-end after a restart to ensure they work properly and are easy to follow.\n",
      "INFO  \n",
      "Source Documents:\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'<!-- toc -->\\n\\n- [Tutorials \"Learn X in 60 minutes\"](#tutorials-learn-x-in-60-minutes)\\n  * [What are the goals for each tutorial](#what-are-the-goals-for-each-tutorial)\\n\\n<!-- tocstop -->\\n\\n# Tutorials \"'}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'- A markdown \\\\`XYZ.API.md\\\\` about the API and the software layer written by us\\n  on top of the native API\\n- A markdown `XYZ.example.md` with a full example of an application using the\\n  API\\n- A Docker'}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {\"- Each Jupyter notebook should\\n  - Be unit tested so that you are guaranteed that it works\\n    - It's super frustrating when a tutorial doesn't work because the version of\\n      the library is not com\"}\n",
      "INFO  File: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Excerpt: {'This is the same approach we use in DATA605 tutorials\\nhttps://github.com/gpsaggese/umd\\\\_data605/tree/main/tutorials, e.g.,\\n\\n- Git\\n- Docker\\n- Docker compose\\n- Postgres\\n- MongoDB\\n- Airflow\\n- Dask\\n- GitH'}\n"
     ]
    }
   ],
   "source": [
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "_LOG.info(\"Answer: %s\", result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "_LOG.info(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    _LOG.info(\"File: %s\", {doc.metadata['source']})\n",
    "    _LOG.info(\"Excerpt: %s\", {doc.page_content[:200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ca5b-2a99-4f29-b349-a4f45b30a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa6d91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
